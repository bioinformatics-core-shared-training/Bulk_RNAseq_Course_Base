In this course, we are going to be using inferential statistics. What does that mean? With inferential statistics, you take data from samples and make generalizations about a population. Once you have a question that you want to pursue with a statistical enquiry, the first thing that you need to think about is the population. What is the population that I am considering in my experiment from which I will gather measurements? If you are working on breast cancer then your population of interest would be all breast cancer patients. Or if you are working with Drosophila Melanogaster, your population would be all drosophila melanogaster. Your sample will be then a smaller set of individuals that represent this population and that you can measure your variable of interest.
This is actually quite challenging - inferring something with only a handful of samples. And this is where statistical distributions are useful.

To convert this question into a statistical framework, we need to think about the null and alternative hypothesis. Null hypothesis is what we are willing to assume is the case until proven otherwise. It's negative, it denies change. In this case, it says drug has no effect on response time. The alternative hypothesis is then the drug has an effect. If we convert it to statistical parameters, the null hypothesis says that the population mean of rats on drug is equal to population mean of rats without drug, which is 1.2 seconds. Under the null hypothesis, we can assume th

Type I error is easy - when you decide to do a statistical test, you need to decide before hand what your significance threshold would be. At what error rate you will reject the null hypothesis - 0.05 is a common choice depending on the field. This value is actually your probability of rejecting the null hypothesis even if it's true. In this plot, your significance levels is the red shaded area, and you took a sample and its mean falls into this shaded area, then you reject the null hypothesis. And if your null hypothesis is actually true, you would be committing a Type I error without knowing about it.

Suppose in another world, the alternative hypothesis is true and there are actually two sampling distributions. Red one is for rats on drug, and the black one is rats without drug. You draw a sample of 100 rats, and the sample mean falls here. Your sample might be genuinely coming from this red distribution, but since it's not falling into the red shaded area you will fail to reject null hypothesis. Hence, you will commit a type II error.

Power is directly associated with Type I error, if you nudge this line to the left, in other words, if you increase your Type I error, you increase your power. Another way to boost power is to increase sample sizes, if you have larger sample sizes these sampling distributions would be steeper, the variance would be lower. There are other factors that we can't control and that have an effect on power as well, such as effect size. In this example, if effect size had been larger, ie. if these means had a higher difference we would have higher power.

So these 4 four concepts effect size, sample size, significance level and power are linked to each other. And if you know one of them, you can work out the forth one. Common uses of theis interdependency is to calculate the power of your study to detect an effect size of interest at a certain significance level with a certain sample size. Or another common usage is to determine the sample size required to detect a certain effect size with a certain significance level within a given power.

Power analysis exist for differential expression analysis as well. For differential expression analysis, I can refer you to this figure in which the authors performed power analysis with varying sequencing depths and varying number of replicates. You can see the trend flattens as we increase the sequencing depth, and what really affects the power is the number of replicates that you are using. If you are using 3 replicates with decent sequencing depths, you only achieve a power of 0.4. The power of 40% means that with 3 biological replicates the probability of finding an effect when there is an effect is 40%.

5 Minute break?

When we do differential expression, we need a more versatile statistical model beyond two condition comparisons. Often we have multiple covariates, multiple indepdendent variables that are associated with gene expression, and we want to incorporate those in a formal statistical framework. And that's why we use linear models. You can do simple two contrast comparions (you can compare treatment vs control), but the same framework also allows you to construct more complex models when you have multiple independen variables such as treatment, age, sex or batch.

In a very layman notation, this is the kind of formula we are after. We want to explain y, which is the expression of gene with a combination of independent variables. In other words, we take our observation and explain it what we know about the observation, the deterministic part of what

We model RNA-seq data with a negative binomial distribution with mean mu and a dispersion parameter phi. And mean here is expressed as s time 2 to the power of X beta. It's the design matrix and the parameter vector. Here we model the mean 2 to the power of x-beta, and the reason for that we want force the estimate of mean to be a positive value. This is mainly for visualization purposes. And another component here is s, which is the scaling factor. As Ash mentioned, DESEQ2 doesn't work with transformed counts but it rather incorporates library size difference when modeling the count data. This is a great property of negative binomial distribution.

After we fit the model, the coefficients for beta vector are estimated along with their standard error. And these coefficients are the estimates for log2-fold changes and will be used as input for hypothesis testing.

So, if we think about this simple linear regression model where we


Each time you use a statistical test, it comes with a set of assumptions. You use DESEQ2 and you model gene expression as negative binomial you are making quite big assumptions. You are assuming all 22 thousand genes are following a negative binomial distribution across all conditions. It's quite a big assumption.

When you analyze the log-fold changes of your favourite genes, just remember that you're making these huge assumptions to infer these log-fold changes. These methods clearly work but know that they are not magical, they are not undisputable and that there are sets of assumptions behind them that might or might not hold true for all genes for all conditions. Take your differential expression analysis results with a critical eye.
