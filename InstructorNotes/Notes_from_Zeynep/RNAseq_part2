We are going to continue with two sessions in the afternoon. We will continue with our aligned reads and look at the quality of the alignment. And after this session, Ash will take over and show us how we can go from these alignments t o gene expression.

We already looked at the quality of the sequence data, and we did alignment. We now want to see how good that alignment is. QC is important in bionformatics, because even with bad data you will get a bam file. RNA-sequencing is a multi-step process, from the cell you extracted cDNA from to the reads that you see in your computer, there are multiple steps that can go wrong. So you need to make sure the data is of good quality before you invest your time and energy into downstream analysis.

There are a number of different metrics we check with aligned reads. We are going to look at 4 particular alignment metrics that are particularly useful for testing the quality of RNA-seq data. If you are sequencing DNA or chromatin, then you might need to look at other metrics, but these are the ones that we find useful. First one is the alignment rate: basically how many of our reads are mapping to our genome. Duplication rate has to do with duplicate reads, we will talk more about that in a minute. We are also going to look at genomic location of the reads. Given that we are working with RNA-seq data, we expect to see most of our reads should come from coding regions. If we had been working with DNA, we would expect a uniform coverage over the entire genome; introns and exons alike, but we are working with with mRNA, so we expect to see mainly exons. So we can access if we had a good library prep based on where our reads are. And finally, we are going to look at transcript coverage. Basically we are going to look at across all the transcripts, how is the coverage across the length of the transcript. The way the fragmentation works, we expect to see reads all along the transcript, so we sort of expect an even coverage. So we can access the quality of our RNA by looking at this.

So, alignment rates. Alignment rates are affected by two things primarily: 1. Quality of the reference genome. For mouse and human we have very good quality reference genomes. So most of the reads we sequence, should match to the reference, there shouldn't many reads that don't match the reference. We are working with mouse in this course, so we expect to see alignment rates of around 95% or more. If you have non-model organism without a good quality reference genome, you might find that more of your reads aren't aligning to the reference genome even though the reads are geniune reads from geniune transcripts  simply because the reference is incomplete. The other thing that can affect your alignment rate is quality of sequences, quality of sequencing. So if you have good quality library prep and good sequencing, you should get high rates of alignment. As a rule of thumb, for human and mouse, we expect to see at least 95% alignment rates, usually we see close to 99% of alignment rates. And for other species, you need to take the reference quality into consideration. In one of my experiments, we sequenced some human melanoma cell lines - fastqc didn't raise any issues, we had good quality sequence data, but alignment rates were around 40-50%, despite having high quality data. We took some of the unmapped reads, and used BLAST to identify their origin, and they matched to bighorn sheep. No one in the institute was working with livestock samples, and to this day it's a mystery to me as how the samples were contaminated. But I have high quality bighorn sheep mRNA data.

The second metric is duplication rate. I mentioned duplication before but where do we get the duplication exactly. There are a couple of sources of duplication with main source being the PCR step during library prep. During the library prep what we aim to do is to get 1 read from each transcript. So each transcript will give us one read, and we need to get that read on to the flowcell. The reads will compete for space on the flowcell, so to increase the chance of actually sequencing 1 read per fragment, we create multiple copies of the same read via PCR.   
The other source of duplicate reads is due to the nature of RNA-seq data. Human exome is 30 megabases, so if every gene is transcribed, we would have 30M possible reads. Since not all the reads are expressed at the same time, we have less than 30M possible reads. So if we sequence 40M reads, we expect to see some duplicates. Whereas duplication rate is a cause of concern for other sequencing experiments, in RNA-seq we expect to see high duplication rates. And with higher depths we expect to see higher duplication rates. We would only be alarmed if see extreme duplication rates, like 80-90%.

We are also going to look at the location of our reads. We are sequencing from the mRNA, so we expect to see most of our reads coming from exons or UTRs. Using gene annotations, we can calculate the percantages of reads coming from different locations along the genome. We will generate such a plot in the practical, and we expect to see this sort of pattern. Mostly coding, and then UTR, and a small percantage in intronic and intergenic. This of course will depend on the quality of the reference genome and annotation, so with human and mouse, this is what we expect, but it might be different for other species. So we still get some intergenic and intronic reads, it can be multiple sources for that: it can be contamination from DNA from your sample, it can be misaligned reads, it can also be error in your annotation - you might be missing some genes in your annotation. So there might be various reasons for that, but this is quite a typical pattern for human and mouse RNA-seq data. If you're not careful with your library prep, you might get contamination with DNA, and it will increase the intergenic-intronic ratios.

The final metric is transcript coverage. What we are doing basically is that we are taking every single transcript, looking at the coverage at each nucleotide of that transcript and we are looking at the average coverage across all transcripts. Then we normalize all our transcripts to a standard length. So what we are looking is how average coverage along the length of the transcript changes. What we expect to see is fairly flat curve. In the beginnning we are ramping up and at around 10% it flattens, and towards the end it drops off. And that's what we expect, it just the random possible allocation of reads we are getting from a transcript. The second one there is a 3' bias, we are getting more coverage towards the end. We will discuss a bit more at the end of the transcript, but just think about it, what might be the biology behind this. This is not extremly alarming, but something you need to be aware of.

Let's do exercise 1 and 2.1 in 10 minutes and get back together.
And for the rest of the exercises, we can do it in 15 minutes maybe?
