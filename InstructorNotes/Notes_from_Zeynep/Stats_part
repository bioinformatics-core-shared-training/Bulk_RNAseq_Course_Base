I just wanted to take this opportunity to convince you that you need statistics in your research life.

For generations, we learned statistics in a very dry way. We learned a set of techniques to be applied in different situations with more focus on the mathematical theory than understanding why that specific technique is used there and what challenges arise when trying to use data to answer questions.

This traditional view of statistics as a bag tools is now facing major challenges. We are in the age of data science now. We are collecting unprecedented amounts of data in every field. And it's not just marketing data from Amazon purchases or social media posts that I'm talking about but biomedical data as well. If you think about this course, we are working with mRNA quantification. Existence of mRNA was finally proven in 1961, so it was just 60 years ago we could isolate mRNA, and now we are able to routinely profile whole mRNA repertoire of any cell. We can even do this in single-cell level. We can even do that in single-cell level for whole organs or whole organisms. More data means that we need to be even more aware of what the evidence is actually worth.

This ties well with the concept of data literacy. It refers to the ability to not only carry out statistical analysis on real-world problems, but also to understand and critique any conclusions drawn by others on the basis of statistics. With this course, you will be equipped with concepts and tools for you to analyze your own RNA-seq data, but also you will gain the ability to have a critical eye on any published RNA-seq data. We are answering more and more biomedical problems with data these days, be it sequencing or imaging or mass spec. And to be able to maintain a critical eye on any published research paper, you need statistics.

And what I mean by that, is that you need statistics as an investigative process of problem-solving and decision making. To formalize that, I want to briefly introduce the PPDAC cycle. When you want to answer a problem with data, what you actually do is to follow a statistical enquiry circle. And this is actually the framework that we follow or need to follow in any data analysis project. This is the framework we are following in this course/  And I'll demonstrate it with the case study we are working on for this course. As mentioned before, we are working with the data generated by Hu and collegaues.

And our cycle starts with a problem. Here the problem is toxoplasma infection is causing severe neurological disorders in some invididuals, but our understanding of the molecular mechanisms associated with infection is incomplete.

...

So this was just my propoganda for statistics - you actually need and use statistical concepts in your data driven research projects, and the complex looking formula containing part is just one component. And we will now talk about that component.

In this course, we are going to be using inferential statistics. What does that mean? With inferential statistics, you take data from samples and make generalizations about a population. Once you have a question that you want to pursue with a statistical enquiry, the first thing that you need to think about is the population. What is the population that I am considering in my experiment from which I will gather measurements? If you are working on breast cancer then your population of interest would be all breast cancer patients. Or if you are working with Drosophila Melanogaster, your population would be all drosophila melanogaster. Your sample will be then a smaller set of individuals that represent this population and that you can measure your variable of interest.
This is actually quite challenging - inferring something with only a handful of samples. And this is where statistical distributions are useful.

To convert this question into a statistical framework, we need to think about the null and alternative hypothesis. Null hypothesis is what we are willing to assume is the case until proven otherwise. It's negative, it denies change. In this case, it says drug has no effect on response time. The alternative hypothesis is then the drug has an effect. If we convert it to statistical parameters, the null hypothesis says that the population mean of rats on drug is equal to population mean of rats without drug, which is 1.2 seconds. Under the null hypothesis, we can assume th

Type I error is easy - when you decide to do a statistical test, you need to decide before hand what your significance threshold would be. At what error rate you will reject the null hypothesis - 0.05 is a common choice depending on the field. This value is actually your probability of rejecting the null hypothesis even if it's true. In this plot, your significance levels is the red shaded area, and you took a sample and its mean falls into this shaded area, then you reject the null hypothesis. And if your null hypothesis is actually true, you would be committing a Type I error without knowing about it.

Suppose in another world, the alternative hypothesis is true and there are actually two sampling distributions. Red one is for rats on drug, and the black one is rats without drug. You draw a sample of 100 rats, and the sample mean falls here. Your sample might be genuinely coming from this red distribution, but since it's not falling into the red shaded area you will fail to reject null hypothesis. Hence, you will commit a type II error.

Power is directly associated with Type I error, if you nudge this line to the left, in other words, if you increase your Type I error, you increase your power. Another way to boost power is to increase sample sizes, if you have larger sample sizes these sampling distributions would be steeper, the variance would be lower. There are other factors that we can't control and that have an effect on power as well, such as effect size. In this example, if effect size had been larger, ie. if these means had a higher difference we would have higher power.

So these 4 four concepts effect size, sample size, significance level and power are linked to each other. And if you know one of them, you can work out the forth one. Common uses of theis interdependency is to calculate the power of your study to detect an effect size of interest at a certain significance level with a certain sample size. Or another common usage is to determine the sample size required to detect a certain effect size with a certain significance level within a given power.

Power analysis exist for differential expression analysis as well. For differential expression analysis, I can refer you to this figure in which the authors performed power analysis with varying sequencing depths and varying number of replicates. You can see the trend flattens as we increase the sequencing depth, and what really affects the power is the number of replicates that you are using. If you are using 3 replicates with decent sequencing depths, you only achieve a power of 0.4. The power of 40% means that with 3 biological replicates the probability of finding an effect when there is an effect is 40%.

5 Minute break?

When we do differential expression, we need a more versatile statistical model beyond two condition comparisons. Often we have multiple covariates, multiple indepdendent variables that are associated with gene expression, and we want to incorporate those in a formal statistical framework. And that's why we use linear models. You can do simple two contrast comparions (you can compare treatment vs control), but the same framework also allows you to construct more complex models when you have multiple independen variables such as treatment, age, sex or batch.

In a very layman notation, this is the kind of formula we are after. We want to explain y, which is the expression of gene with a combination of independent variables. In other words, we take our observation and explain it what we know about the observation, the deterministic part of what

We model RNA-seq data with a negative binomial distribution with mean mu and a dispersion parameter phi. And mean here is expressed as s time 2 to the power of X beta. It's the design matrix and the parameter vector. Here we model the mean 2 to the power of x-beta, and the reason for that we want force the estimate of mean to be a positive value. This is mainly for visualization purposes. And another component here is s, which is the scaling factor. As Ash mentioned, DESEQ2 doesn't work with transformed counts but it rather incorporates library size difference when modeling the count data. This is a great property of negative binomial distribution.

After we fit the model, the coefficients for beta vector are estimated along with their standard error. And these coefficients are the estimates for log2-fold changes and will be used as input for hypothesis testing.

So, if we think about this simple linear regression model where we


Each time you use a statistical test, it comes with a set of assumptions. You use DESEQ2 and you model gene expression as negative binomial you are making quite big assumptions. You are assuming all 22 thousand genes are following a negative binomial distribution across all conditions. It's quite a big assumption.

When you analyze the log-fold changes of your favourite genes, just remember that you're making these huge assumptions to infer these log-fold changes. These methods clearly work but know that they are not magical, they are not undisputable and that there are sets of assumptions behind them that might or might not hold true for all genes for all conditions. Take your differential expression analysis results with a critical eye.
