This course is going to take you through the analysis of bulk RNA sequence data and more specifically the analysis differential gene expression. There are quite a lot of other things that you can do with RNA-seq data, but they are kind of course in themselves and there are even other types of hight throughput sequence data. And in the future if you find yourselves with other types of data, it's always worth checking if there are courses available through
If there are no courses available it's always worth dropping them an email to see if there is enough demand to provide a course.

We are just going to go ahead with the differential gene expression. These kind of experiments have mainly 4 steps. Experimental design, library preparation, sequencing and analysis. We will spend most of our time in this course with bioinformatics analysis, but I want to stress that actually the most important part of this workflow is the experimental design. The bioinformatics analysis is largely context agnostic, it will take your input and will give you an output. It is up to you to make sure what you're asking can be answered with the data that you generate. Even with a bad design you can obtain a list of differentially expressed genes, but that list won't be the reflection of true biology.

We will have a dedicated session with Abbie tomorrow. We're going to talk more in depth about this tomorrow but in essence you have to keep in mind these criteria when you are thinking about your experiment.

There are practical considerations

how many reads do you need
what is the read length do you need
and what library preperation method

What is a batch effect? I'm going to explain it with an example. In 2007 this was paper published in Nature Genetics that looked for differences in gene expression in different ethnic groups: individuals with centreal european ancestry versus individuals with asian ancestry. They identified that 78% percent of genes were differentially expressed between two ethnic groups. You can see in the p-value histogram in the lower left that there are tons of tiny p-values, so there is a lot of genes that look like differentially expressed between the two groups. This seems like a big and important result because there is actually very little genetic variation between ethnic groups, so it's surprising that majority of the genes are differentially expressed.

Another group had a second look at this data, and it turns out when the data were collected all of the samples that came from Europeans were collected primarily between 2003 and 2004, while samples from individuals with Asian ancestry were collected later. So with this information they re-analyzed the data and looked for gene expression differences between the years and found that 96% of the genes are differentially expressed. And once you adjust for the fact that the samples were taken in different years, all the difference between the population goes away. So this is what is called a batch effect. It basically suggests that there is a confounder and that is the date the samples are taken. Why would date matter? The technology might change, assays might change, extraction protocols might change, there can be a number of reasons why date might be associated with differential expression.

In a formal definition, batch effects are sub-groups of measurements that have qualitatively different behaviour across conditions and are unrelated to the biological or scientific variable in a study. It's seeing correlations when we don't expect to see them. They become especially problematic if they are confounded with the experimental variable, however if they are randomly distributed across experimental variable they can be controlled for.

So what you need to do is to randomise all technical steps on data generation in order to avoid batch effects. When you are making plate based operations, if you're doing RNA extraction on plate or library prep on plate, suppose that you have 4 different conditions. It's intuitive or maybe practical to design the plate like this, with your groups nicely seperated. This might and quite possibly will give you batch effect. It's known that PCR efficiency is variable in plate based library prep protocols, you might have drop in efficieny on the corners. Green samples and red samples might have variations, than it becomes very hard to control it. What you need to do is to randomize your plate design.

In another example suppose you have a study where you have 6 replicates for treatment and control. And you process your samples in batches of 4 because that's what you can manage on bench. What you don't want is have all your controls in one batch, and all your treatments in one batch. This will lead to classical batch effect - you need to balance your batches and you should have same amount of replicates in each batch. It might not always possible to get exactly the same amount of replicates in each batch, but you need as close as possible.

Next thing is multiplexing. When we do sequencing, we combine multiple samples in a sequencing lane, this is essentially what it is called multiplexing. In this toy example, there are two samples. The reads extracted from these two samples will get two different barcodes, two different index primers. So you can multiplex them before sequencing, and after sequencing you can demultiplex them using these unique indices.
This was a massive improvement when it was introduced, because people were sequencing different samples on different runs and this was causing variation. Now we can put the samples together and avoid this issue.

And then there are other hidden confonding variables as well. So when you are designing your experiment, think carefully about all the underlying variables that exist. It could be age, sex, litter, cell passage, or other things. And the best solution is to record everything - even if you think it's not important or directly associated, keep a record of it. If later on you see some funny stuff in your data, you can go back and check if it's related to any other underlying variable.

After you design the experiment you need to decide on library preparation. In a standard bulk RNA-seq setting, there is one decision really: total RNA or polyA selection. This is because ribosomal RNA makes up 80% or RNA in a cell. So if you don't do any ribosomal RNA removal prior to sequencing, 80% of your data will be ribosomal RNA.

There are two options for removing ribosomal RNA: poly-A selection and ribominus selection. In poly-A selection, you specifically select RNA species that have a polyA tail. In ribominus selection, you deplete anything that looks like ribosomal RNA  via homology. So these are two different methods resulting in slightly different enrichments in terms of the RNA species captured. Poly-A selection is very efficient for capturing mature mRNAs but also immature miRNAs and small nuclear RNAs. While with ribominus in addition to these, you can capture transfer RNAs, mature miRNAs, piRNAs. So if you are looking for a whole range of RNA species, ribominus would be your choice - but if you are mainly interested in mRNA or gene expression you would go with polyA selection. I personally only worked once with ribominus selection, where we were interested in miRNAs and mRNAs simultaneously, but except for that one study I worked with polyA data.

Once you decide on your selection method, you will go ahead with library preparation. So let's start with library preparation. I want to start off saying that we are using Illumina protocol and sequencer, because they are commonly used. But keep in mind that there are other protocols and sequencers, and there might be differences.

This graph illustrates the polyA selection at step 1 but it's essentialy same for ribominus selection. Once you have your fragments, you break them up into smaller fragments them because the sequencing technologies can only deal with short reads, and these mRNA fragments can be quite long . After fragmentation you convert your RNA  fragments to double stranded DNA, and this is necessary since double stranded DNA is more stable than RNA and can be easily amplified and modified.

Then you add your sequencing adapters - these adapters will do two things 1. they will allow sequencing machine to recognize the fragments, and 2. they will allow you to sequence different samples at the same time, since different samples can use different adapters.

And in the last step, we PCR amplify the library.

After you prepare your library, you will go ahead with sequencing. Illumina uses a technique called sequencing by synthesis, and some of you might be familiar with how this works but I'll give just a brief overview on how this works as it affects how we analyze the data. In this toy example, we have 4 fragments in our flowcell. Actually, there are about 400M fragments laid out vertically in a typical flowcell. In our toy example, we have four here.

The sequencing machine has fluorescent labeled bases that are color coded according to the type of nucleotide they can bind to. The bases are incorporated to the first base of each fragment, and once the bases are incorporated, the machine takes a picture above that looks like this. Then the colors are washed off and the bases are incorporated again, and an image taken. So this cycle of wash - addition- imaging continues until each fragment is sequenced completely.

Since we know which color corresponds to which base, and we know the location of the probes, we can decipher the sequence. This is how it works with 4 fragments, with 400M fragments the matrix is much denser. This matrix is still not dense enough, but it illustrates one type of problem that can occur. Sometimes a probe does not shine as bright as it should and the sequencer can not be confident that it is calling the correct color. Quality scores, that are part of the output, reflect how confident the machine is that it correctly called a base. In this case, the faded dot will get a low quality score.

All this information is outputted by the sequencer and will be the input for our bioinformatics analysis. So we are now ready for bioinformatics.

When you get your sequence data it is going to come in the form of fastq file. Fastq file is essentially is a big text file. You have 4 lines for each read. The first line is read name, this is a unique identifier for each individual read. The second line is the actual sequence, actual base calls for each of the reads. The third line is again the read name, you don't need that. 4th line is quality scores for each base. As we have seen with in the previous presentation, these score reflect how confident the machine is that it correctly called a base. These scores are ASCI encoded, because we need a single character representation of the scores for each base, if we use multiple characters it would get very confusing. So for this reason, the quality scores are encoded in these alphanumeric characters.

QC is super important, I think you will hear this in multiple times today. We will use a super popular program called FastQC for assessing the quality of our sequence data. It's super quick and convenient. It will give us an idea if our sequencing worked ok, if there are issues that we need to address before going ahead with analysis.

When you get the fastqc results, it will give you a number of graphs. It's important to note that Fastqc was originally developed for genomic sequence data, so not all the reports are relevant for RNA-seq data, and I will show 4 of them that are relevant for RNA-seq data, so these are the ones that I check in my data.

First one is per base sequence quality graph. This is a visual representation of the quality of your reads - on the y-axis we have the quality scores and the x-axis is the length of the sequence read. In this case, we are looking at a 50bp read, and at each base position, we create a boxplot which shows you the range of quality scores. With good data, you don't even see the boxplots. Whereas with the bad data, quality score has a huge variation and it even goes very low quality towards the ends. So, this data can be salvaged with trimming based on quality - which is covered in the extended materials. Quality score 20 means that there is 1 in 100 chance that the base call is wrong.

Another metric is the per base sequence content. This is a representation of the amounts of each base in each position and you would expect this to be fairly consistent along the read. You might be alarmed with the spiky behaviour at the beginning of the reads of the good data, but that is totally normal and standard in RNAseq data. Bad data example is very spiky and for instance at the first base half of you reads are G, in the second base it's A, you can almost read the sequence. This might be due to ribosomal RNA contamination, contamination from other RNA species, so this kind of output is bad news usually - it indicates a problem with your library content.

Next one GC content - you expect to see a kind of Gaussian distribution according to the GC content of your species. The good data example here is very good, it doesn't always look this good but some kind of a Gaussian curve is expected. The bad data example kind of indicates two different GC distributions - this might be a contamination from another species altogether. AS you know different species will have different GC content - so seeing this bimodal distribution might indicate that. So if you know what the contaminating species is, you might rescue this by mapping the data to the other species first and using the remaining the data for your analysis.

The last one is adapters. You might have this problem if you had issues in the library prep - if your fragments are very short, when it comes to the flowcell, it will sequence the adapters.
